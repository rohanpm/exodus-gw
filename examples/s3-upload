#!/usr/bin/env python3
# Upload a list of files from the command-line to s3, possibly
# using a custom endpoint.
#
# This command is used as a baseline for testing s3 uploads
# via exodus-gw or via s3 directly. It can be used to compare
# performance and compatibility.
#
# Usage:
#
#  # Using default S3
#  examples/s3-upload file1 [file2 [...]]
#
#  # Testing same thing via exodus-gw
#  uvicorn exodus_gw.gateway:app &
#  examples/s3-upload --endpoint-url http://localhost:8000/upload file1 [file2 [...]]
#
# It is recommended to test using a mixture of files both smaller
# and larger than 10MB to cover both multipart and single part
# uploads.

import argparse
import hashlib
import logging
import sys

import boto3


def get_object_key(filename):
    with open(filename, "rb") as f:
        hasher = hashlib.sha256()
        while True:
            chunk = f.read(1024 * 1024 * 10)
            if not chunk:
                break
            hasher.update(chunk)
        return hasher.hexdigest()


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--debug", action="store_true", help="Enable verbose logging"
    )
    parser.add_argument(
        "--endpoint-url", default="https://localhost:8010/upload"
    )
    parser.add_argument("--env", default="dev")
    parser.add_argument("files", nargs="+")

    args = parser.parse_args()

    if args.debug:
        logging.basicConfig(level=logging.DEBUG)

    s3 = boto3.resource("s3", endpoint_url=args.endpoint_url)
    bucket = s3.Bucket(args.env)

    print(
        "Using endpoint:",
        "[default]" if not args.endpoint_url else args.endpoint_url,
    )

    for filename in args.files:
        bucket.upload_file(filename, get_object_key(filename))
        print("Uploaded:", filename)


if __name__ == "__main__":
    main()
